---
# title: "Publications"
permalink: /papers/
layout: single
author_profile: true
---

<!-- ## Publications -->

<!-- #### Attend First: Consolidate Later: On the Importance of Attention in Different LLM Layers  
BlackBox NLP 2024 
Amit Ben-Artzy, Roy Schwartz  
<div style="margin-top: 0.5em;">
  <a href="https://arxiv.org/abs/2409.03621" class="btn btn--custom">ArXiv</a>
  <a href="https://x.com/Amit_BenArtzy/status/1854155692363416010?t=JGwEL7S4H_-crJi_PhN1dw&s=19" class="btn btn--custom">Tweet</a>
  <a href="https://github.com/schwartz-lab-NLP/Attend-First-Consolidate-Later" class="btn btn--custom">Code</a>
</div>   -->


<div class="citation">
  <h3 class="archive__item-title" style="margin-bottom: 0.2em;">Attend First: Consolidate Later: On the Importance of Attention in Different LLM Layers</h3>
  <p class="archive__item-excerpt" style="margin-bottom: 0.2em;">
    <em>BlackBox NLP 2024</em><br>
    Amit Ben-Artzy, Roy Schwartz
  </p>
  <div style="margin-top: 0.5em;">
    <!-- <a href="https://arxiv.org/abs/2409.03621" class="btn btn--custom"><i class="fa-solid fa-file"> </i>ArXiv</a>
    <a href="https://x.com/Amit_BenArtzy/status/1854155692363416010?t=JGwEL7S4H_-crJi_PhN1dw&s=19" class="btn btn--custom"><i class="fa-brands fa-twitter"></i> Tweet</a>
    <a href="https://github.com/schwartz-lab-NLP/Attend-First-Consolidate-Later" class="btn btn--custom"> <i class="fa-brands fa-github"> </i>Code</a> -->
    <a href="https://arxiv.org/abs/2409.03621" class="btn btn--custom">ArXiv</a>
    <a href="https://x.com/Amit_BenArtzy/status/1854155692363416010?t=JGwEL7S4H_-crJi_PhN1dw&s=19" class="btn btn--custom">Tweet</a>
    <a href="https://github.com/schwartz-lab-NLP/Attend-First-Consolidate-Later" class="btn btn--custom"> Code</a>
  </div>
</div>


  <!-- <a href="https://arxiv.org/abs/2409.03621" class="btn--custom">
  <i class="/assets/images/pdf.png"></i> arXiv
</a> -->